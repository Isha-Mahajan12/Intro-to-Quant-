[
  {
    "path": "posts/2022-04-03-homework-1/",
    "title": "Homework 1: Function Writing, Variation",
    "description": "Gelman Chapter 6",
    "author": [
      {
        "name": "Isha Akshita Mahajan",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-04-03",
    "categories": [],
    "contents": "\nSetup Code:\n\n\nhide\n\n#load packages tidyverse and rstanarm\nlibrary(tidyverse)\nlibrary(rstanarm)\n#load earnings dataset from desired file path\nearnings <- read.csv(\"earnings.csv\")\n\n\n\n6.2 Programming Fake-Data Simulation: Write an R function to:\nsimulate n data point from the model, y= a+bx+error,with data points x uniformly sampled from the range(0,100) with errors drawn independently from the normal distribution with mean 0 and standard deviation σ*\nfit a linear regression to the simulated data*\nmake a scatterplot of the data and fitted line\n\n\nhide\n\nset.seed(3)\n#Create a function for a fake data simulation\nfake_data <- function(a,b,sigma,n)#Think of this as variables\n#The curly brackets are what inputs go into and what outputs they generate\n{\n#create object x and define it using runif() function\nx <- runif(n,0,100)\n#add in simple regression \ny <- a+b*x+sigma*rnorm(n)\n\nrandom <- data.frame(x,y)\n\n#Use Stan_glm function to fit the model\nfitted_random <- stan_glm(y~x, data = random)\n\n#use print to display results concisely \nprint(fitted_random, digits=3)\n\nplot(random$x,random$y,main = \"Data generated and fitted regression line\")\na_hat<- coef(fitted_random) [1]\nb_hat <- coef(fitted_random)[2]\nabline(a_hat, b_hat)\n\nx_bar <-mean(random$x)\ntext(x_bar, a_hat + b_hat*x_bar,\npaste(\"y =\",round(a_hat,2),\"+\",round(b_hat, 2), \"* x\"), adj=0) }\n\n\n\n\n\nhide\n\nset.seed(3)\nfake_data(0.4,0.2,0.5,100)\n\n\n\n\n#6.3 Variation, uncertainty, and sample size: Repeat the example in Section 6.2, varying the number of data points, n. What happens to the parameter estimates and uncertainties when you increase the number of observations?\n\n\nhide\n\nset.seed(3)\nfake_data(0.4,0.2,0.5,175)\n\n\n\n\n\n\nhide\n\nset.seed(3)\nfake_data(0.4,0.2,0.5,215)\n\n\n\n\n\n\nhide\n\nset.seed(3)\nfake_data(0.4,0.2,0.5,275)\n\n\n\n\nAs I ran each code chunk by increasing the value of n in my sample, I observed that the MAD_SD values or the standard deviation started reducing. This suggests that a larger sample size leads to less error\n#6.5 Regression prediction and averages: The heights and earnings data in Section 6.3 are in the folder Earnings. Download the data and compute the average height for men and women in the sample.\nAssuming 52% of adults are women, estimate the average earnings of adults in the population.\nDirectly from the sample data compute the average earnings of men, women, and everyone. Compare these to the values calculated in parts (a) and (b)\n\n\nhide\n\n#look at the columns in the dataset\nhead(earnings)\n\n\n  height weight male  earn earnk ethnicity education mother_education\n1     74    210    1 50000    50     White        16               16\n2     66    125    0 60000    60     White        16               16\n3     64    126    0 30000    30     White        16               16\n4     65    200    0 25000    25     White        17               17\n5     63    110    0 50000    50     Other        16               16\n6     68    165    0 62000    62     Black        18               18\n  father_education walk exercise smokenow tense angry age\n1               16    3        3        2     0     0  45\n2               16    6        5        1     0     0  58\n3               16    8        1        2     1     1  29\n4               NA    8        1        2     0     0  57\n5               16    5        6        2     0     0  91\n6               18    1        1        2     2     2  54\n\nhide\n\n#base R rules dataframe$column\n#Find the means \nmen_mean <- mean(earnings$height[earnings$male==1]) \nwomen_mean <- mean(earnings$height[earnings$male==0])\n\n\n\n70.089 for men, 64.490 for women\nUse these averages and fitted regression model displayed on page 84 to get a model-based estimate of the average earnings of men and of women in the population.\n\n\nhide\n\nset.seed(3)\n#Use Stan_glm function to fit the model\nfitted_regression <- stan_glm( earn ~ height + male, data= earnings)\n\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 6.2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.62 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.705958 seconds (Warm-up)\nChain 1:                0.181535 seconds (Sampling)\nChain 1:                0.887493 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.512607 seconds (Warm-up)\nChain 2:                0.169639 seconds (Sampling)\nChain 2:                0.682246 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 1.04908 seconds (Warm-up)\nChain 3:                0.199407 seconds (Sampling)\nChain 3:                1.24848 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.4e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.33277 seconds (Warm-up)\nChain 4:                0.199589 seconds (Sampling)\nChain 4:                0.532359 seconds (Total)\nChain 4: \n\nhide\n\n#use print to display results concisely \nprint(fitted_regression, digits=2)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      earn ~ height + male\n observations: 1816\n predictors:   3\n------\n            Median    MAD_SD   \n(Intercept) -26037.06  12121.78\nheight         649.09    187.83\nmale         10567.39   1456.63\n\nAuxiliary parameter(s):\n      Median   MAD_SD  \nsigma 21398.62   346.92\n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n\n",
    "preview": "posts/2022-04-03-homework-1/HW1_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-04-23T11:35:46-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-03-homework-2/",
    "title": "Homework 2: Exploratory Data Analysis",
    "description": "Exploratory Data Analysis on Voter Turnout and Ethnic Data and Gelman Chapter 10",
    "author": [
      {
        "name": "Isha Akshita Mahajan",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-04-03",
    "categories": [],
    "contents": "\nLoad Required Packages\n\n\nhide\n\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(ggplot2)\nlibrary(bayesplot)\nlibrary(gghighlight)\n\n\n\nQuestion 1\nFraga analyzes turnout data for four different racial and ethnic groups, but for this analysis we will focus on the data for black voters. Load blackturnout.csv. Which years are included in the dataset? How many different states are included in the dataset?\n\n\nhide\n\nvoter_data<-read_csv(\"blackturnout.csv\")\nhead(voter_data)\n\n\n# A tibble: 6 × 7\n   ...1  year state district turnout   CVAP candidate\n  <dbl> <dbl> <chr>    <dbl>   <dbl>  <dbl>     <dbl>\n1     1  2008 AK           0   0.710 0.0350         0\n2     2  2010 AK           0   0.448 0.0323         0\n3     3  2010 AK           1   0.448 0.0323         0\n4     4  2008 AK           1   0.710 0.0350         0\n5     5  2006 AK           1   0.439 0.0318         0\n6     6  2010 AL           0   0.397 0.256          0\n\nhide\n\nyears<- voter_data %>% \n select(year) %>% \n count(year)\n\nstates<- voter_data %>% \n  select(state) %>% \n  count(state)\n\n\n\nThis dataset includes data from years 2006,2008 and 2010 and includes information on voter turnout from 42 states\nQuestion 2:\nCreate a boxplot that compares turnout in elections with and without a co-ethnic candidate.Be sure to use informative labels.Interpret the resulting graph.\n\n\nhide\n\nlibrary(ggplot2)\nlibrary(ggthemes)\n\nboxplot<- voter_data %>% \n  group_by(candidate) %>% \n  select(candidate,turnout)%>% \n  mutate(candidate = recode(candidate, `1` = \"Black Candidate\", `0` = \"No Black Candidate\")) %>% \n  \n  ggplot(aes(x=candidate,y=turnout,group=candidate, fill=candidate))+\n  geom_boxplot(show.legend = TRUE)+\n  labs(x=\"Ethnicity\", y= \"Black Voter Turnout\", title = \"Elections With & Without Black Candidates\", subtitle= \"Boxplot for elections with and without the presence of black candidates\", caption=\"Graphic: Isha Akshita Mahajan/ Student,UMass Amherst\\nSource: YouGov\")+\n  theme_minimal()+\n  theme(legend.position = \"top\", legend.box = \"horizontal\")+\n  theme(text=element_text (size = 12),\n  plot.title =element_text(size=rel(1.5)))\n  boxplot\n\n\n\n\nThis boxplot shows the difference between the turnout for elections with and without co-ethnic candidates. The median for black voter turnout is lower for non co ethnic candidates whereas there is a higher turnout in elections where there is presence of co ethnic candidates. This can be considered as a step towards correlation however,this boxplot can not represent causality on it’s own.\nQuestion 3:\nRun a linear regression with black turnout as your outcome variable and candidate co-ethnicity as your predictor. Report the coefficient on your predictor and the intercept. Interpret these coefficients. Do not merely comment on the direction of the association (i.e.,whether the slope is positive or negative). Explain what the value of the coefficients mean in terms of the units in which each variable is measured. Based on these coefficients, what would you conclude about blacks voter turnout and co-ethnic candidates?\n\n\nhide\n\nset.seed(007)\nregression_3 <- lm(turnout~candidate, data = voter_data)\nsummary(regression_3)\n\n\n\nCall:\nlm(formula = turnout ~ candidate, data = voter_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.3164 -0.1282 -0.0436  0.1191  0.5832 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.393857   0.005183  75.993  < 2e-16 ***\ncandidate   0.061640   0.014984   4.114 4.15e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.171 on 1235 degrees of freedom\nMultiple R-squared:  0.01352,   Adjusted R-squared:  0.01272 \nF-statistic: 16.92 on 1 and 1235 DF,  p-value: 4.149e-05\n\nhide\n\n#plot(turnout~candidate, data = voter_data)\n#abline(regression_3)\n\n\n\nThis model summarizes the difference in average black voter turnout between elections that have black candidates (1) and elections that don’t (0). The intercept, 0.39, is the average or predicted proportion of black voting age population in a district that votes in a general election when the election does not include a co-ethnic/black candidate. To average the proportion of black voting age population when the election does include a co-ethnic/black candidate, we would add 0.06 i.e. 0.39+0.06 to get 0.45 which is the predicted turnout of elections with co-ethnic candidates. This tells us that the turnout for election with black candidate is on average, 0.06 units higher than the one without a co-ethnic candidates. This regression has a positive slope and can be interpreted as a summary of the difference in average voter turnout for elections with and without black candidates.\nQuestion 4:\nYou decide to investigate the results of the previous question a bit more carefully because the elections with co-ethnic candidates may differ from the elections without co-ethnic candidates in other ways. Create a scatter plot where the x-axis is the proportion of co-ethnic voting-age population and the y-axis is black voter turnout. Color your points according to candidate co-ethnicity. That is, make the points for elections featuring co-ethnic candidates one color, and make the points for elections featuring no co-ethnic candidates a different color. Interpret the graph.\n\n\nhide\n\nscatterplot<- voter_data %>% \n  group_by(candidate) %>% \n  select(candidate,turnout,CVAP)%>%\n mutate(candidate = recode(candidate, `1` = \"Black Candidate\", `0` = \"No Black Candidate\")) %>%  \n  \n  ggplot(aes(x=CVAP,y=turnout, colour=candidate,fill=candidate))+\n  geom_point()+\n labs(x= \"Proportion of Eligible Voters Who Are Black\", y= \"Black Voter Turnout\", title = \"Elections With and Without Black Candidates \", subtitle = \"When there no black candidates running for elections, the proportion of eligible voters who show up\\n to vote is relatively lower to when there is an election that includes a co-ethnic candidate\",caption=\"Graphic: Isha Akshita Mahajan/ Student,UMass Amherst\\nSource: YouGov\")+\n  theme(legend.position=\"bottom\")+\n  theme_minimal()+\n theme(text=element_text (size = 12,hjust = 0.5),\n       plot.subtitle = element_text(size = 10))\nscatterplot\n\n\n\n\nThis scatterplot shows that when there no black candidates running for elections, the proportion of eligible voters who show up to vote is relatively lower to when there is an election that includes a co-ethnic candidate.This can be considered as an additional step towards analyzing black voter turnout, however, we can not make strong causal claims.\nI would like to expand on Clustering around low CVAP and low turnout but I’m still a little confused on that part\nQuestion 5:\nRun a linear regression with black turnout as your outcome variable and with candidate co-ethnicity and co-ethnic voting-age population as your predictors.Report the coefficients, including the intercept. Interpret the coefficients on the two predictors, ignoring the intercept for now (you will interpret the intercept in the next question). Explain what each coefficient represents in terms of the units of the relevant variables.\n\n\nhide\n\nset.seed(007)\nregression_5 <- lm(turnout~candidate + CVAP, data = voter_data, refresh=0)\nsummary(regression_5)\n\n\n\nCall:\nlm(formula = turnout ~ candidate + CVAP, data = voter_data, refresh = 0)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.30534 -0.12775 -0.04529  0.11750  0.59576 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.375275   0.006677  56.203  < 2e-16 ***\ncandidate   -0.007364   0.021703  -0.339    0.734    \nCVAP         0.207392   0.047497   4.366 1.37e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1698 on 1234 degrees of freedom\nMultiple R-squared:  0.02853,   Adjusted R-squared:  0.02695 \nF-statistic: 18.12 on 2 and 1234 DF,  p-value: 1.756e-08\n\nhide\n\n#plot(voter_data$candidate, voter_data$CVAP)\n\n\n\nFor every one unit of increase in the proportion of district’s voting age population that is black, the black voter turnout increases by 0.21 units, on average, given there are no co-ethnic candidates.In the regression results above, we see that the coefficient for candidate is negative and insignificant (no stars), therefore the value does not hold much relevance in the model. In the earlier regression with only one predictor-turnout, there was an average increase in black voter turnout by 0.06 units, however, with the addition of another predictor,the value of that seemed to have diminished.\nQuestion 6:\nNow interpret the intercept from the regression model with two predictors. Is this intercept a substantively important or interesting quantity? Why or why not?\nThe intercept in an interesting quantity for two reasons:First, the regression results show that it is a significant value therefore it shall be an integral part of the model. Secondly, On average, the intercept is 0.37 which in comparison to the model with one predictor is only 0.02 units lower. This comparison shows that the intercept is in alignment with the fit regardless of the turnout predictor\nQuestion 7:\nRelationship between co-ethnic candidates and black voter turnout. Based on regression model with two predictors, what do you conclude about the relationship between co-ethnic candidates and black voter turnout. Ignore issues of statistical significance.\nKeeping aside the significance, the coefficient value of candidate is -0.007 which is very minuscule and therefore does not have a significant impact on the intercept and the model itself. I thought this to be in alignment with null hypothesis and might contribute towards making an argument for that stronger?\nQuestions From RaOS\n10.2 Regression with Interactions\nWrite the equation of the estimated regression line of y on x for the treatment group and the control group, and the equation of the estimated regression line of y on x for the control group.\nGraph with pen on paper the two regression lines, assuming the values of x fall in the range (0, 10). On this graph also include a scatterplot of data (using open circles for treated units and dots for controls) that are consistent with the fitted model.\n\nPart A\n\nx=1.6 (Pre Treatment Predictor) z= 2.7 (Treatment Indicator) x:z = 0.7 sigma = 0.5\nWhen z=0, then y= a+bx+cz+d(x:z) = 1.2+1.6x+2.7(0)+0.7(x)(0) = 1.2+1.6x\nWhen z=1, then y= 1.6x+1.2+2.7(1)+0.7(x)(1) = 3.9+2.3x\nIn Treatment, for every one unit increase in x, y increases by an average of 0.7 units in control group.\n\nPART B\n\n\n\nhide\n\nx<- runif(100,0,10)\nz<-c(0,1)\nsigma<- rnorm(100)\ny<- 1.2+1.6*x+2.7*z+0.7*x:z+sigma\n\nfakedata<- data.frame(x,y,z) \n\nplot_data <- fakedata %>% \n  mutate(z = recode(z, `1` = \"Treatment\", `0` = \"Control\")) %>% \n  \n  ggplot(aes(x,y,factor(z)))+\n  geom_point(aes(colour=z,fill=z))+\n  geom_abline(slope = 2.3, intercept=3.9)+\n  geom_abline(slope = 1.6, intercept=1.2)+\n  xlim(0,10)+\n  ylim(0,20)+\n  labs(x= \"X\", y= \"Y\", title = \" Regression with Interactions\",subtitle= \"For every one unit of increase in x in treatment, y increase by 0.7 units in contol group on average\", caption= \"Graphic: Isha Akshita Mahajan/ Student,UMass Amherst\\nSource: Fake Data\")+\n  theme_minimal()+\n  theme(legend.position=\"bottom\")+\n  theme(text=element_text (size = 10),\n  plot.title = element_text(size=rel(1.5)))\n\n  plot_data\n\n\n\n\nQuestion 10.3\n\n\nhide\n\nvar1 <- rnorm(1000, 0, 1)  \nvar2 <- rnorm(1000, 0, 1)  \nfake <- data.frame(var1, var2)  \nfit_lm <- lm(var2 ~ var1, data=fake)\nsummary(fit_lm,)\n\n\n\nCall:\nlm(formula = var2 ~ var1, data = fake)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1708 -0.6723  0.0279  0.5928  3.4590 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  0.01832    0.03222   0.569    0.570\nvar1         0.02115    0.03245   0.652    0.515\n\nResidual standard error: 1.019 on 998 degrees of freedom\nMultiple R-squared:  0.0004253, Adjusted R-squared:  -0.0005763 \nF-statistic: 0.4247 on 1 and 998 DF,  p-value: 0.5148\n\n\n\nhide\n\nvar3 <- rnorm(1000, 0, 1)  \nvar4 <- rnorm(1000, 0, 1)  \nfake_1 <- data.frame(var3, var4)  \nfit_lm_1 <- lm(var4 ~ var3, data=fake_1)\nsummary(fit_lm_1)\n\n\n\nCall:\nlm(formula = var4 ~ var3, data = fake_1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2702 -0.6763 -0.0161  0.6761  2.8278 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) -0.01769    0.03142  -0.563    0.574\nvar3         0.03460    0.03185   1.086    0.278\n\nResidual standard error: 0.9934 on 998 degrees of freedom\nMultiple R-squared:  0.001181,  Adjusted R-squared:  0.0001804 \nF-statistic:  1.18 on 1 and 998 DF,  p-value: 0.2776\n\n\n\nhide\n\nvar5 <- rnorm(1000, 0, 1)  \nvar6 <- rnorm(1000, 0, 1)  \nfake_2 <- data.frame(var3, var4)  \nfit_lm_2 <- lm(var6 ~ var5, data=fake_2)\nsummary(fit_lm_2)\n\n\n\nCall:\nlm(formula = var6 ~ var5, data = fake_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.5125 -0.7104  0.0315  0.7026  3.7598 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)  0.034537   0.031892   1.083    0.279\nvar5        -0.005155   0.031275  -0.165    0.869\n\nResidual standard error: 1.008 on 998 degrees of freedom\nMultiple R-squared:  2.722e-05, Adjusted R-squared:  -0.0009748 \nF-statistic: 0.02717 on 1 and 998 DF,  p-value: 0.8691\n\nQuestion 10.6\nRegression models with interactions: The folder Beauty contains data (use file beauty.csv) Beauty and teaching evaluations from Hamermesh and Parker (2005) on student evaluations of instructors’ beauty and teaching quality for several courses at the University of Texas. The teaching evaluations were conducted at the end of the semester, and the beauty judgments were made later, by six students who had not attended the classes and were not aware of the course evaluations.\n\nPART A\n\nRun a regression using beauty (the variable beauty) to predict course evaluations (eval), adjusting for various other predictors. Graph the data and fitted model, and explain the meaning of each of the coefficients along with the residual standard deviation. Plot the residuals versus fitted values.\n\n\nhide\n\nbeauty<- read.csv(\"beauty.csv\")\nfit_beauty<- lm(eval~beauty, data= beauty)\nsummary(fit_beauty)\n\n\n\nCall:\nlm(formula = eval ~ beauty, data = beauty)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.80015 -0.36304  0.07254  0.40207  1.10373 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.01002    0.02551 157.205  < 2e-16 ***\nbeauty       0.13300    0.03218   4.133 4.25e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5455 on 461 degrees of freedom\nMultiple R-squared:  0.03574,   Adjusted R-squared:  0.03364 \nF-statistic: 17.08 on 1 and 461 DF,  p-value: 4.247e-05\n\nhide\n\nplot(fit_beauty)\n\n\n\n\nThe regression above shows that for every one unit increase in beauty, the evaluations increase by 0.13 units, on average. Both the intercept and the beauty variable hold statistical significance in the model.\n\nPART B\n\nFit some other models, including beauty and also other predictors. Consider at least one model with interactions. For each model, explain the meaning of each of its estimated coefficients.\n\n\nhide\n\nbeauty<- read.csv(\"beauty.csv\")\nfit_1<- lm(eval~beauty+female, data= beauty)\nsummary(fit_1)\n\n\n\nCall:\nlm(formula = eval ~ beauty + female, data = beauty)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.87196 -0.36913  0.03493  0.39919  1.03237 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.09471    0.03328  123.03  < 2e-16 ***\nbeauty       0.14859    0.03195    4.65 4.34e-06 ***\nfemale      -0.19781    0.05098   -3.88  0.00012 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5373 on 460 degrees of freedom\nMultiple R-squared:  0.0663,    Adjusted R-squared:  0.06224 \nF-statistic: 16.33 on 2 and 460 DF,  p-value: 1.407e-07\n\nWhen beauty=0, i.e. male, the female coefficient is -0.20 which shows a decrease of 0.20 units in evaluations on average\n\n\nhide\n\nfit_2<- lm(beauty~eval+female+age+female:age,data=beauty)\nsummary(fit_2)\n\n\n\nCall:\nlm(formula = beauty ~ eval + female + age + female:age, data = beauty)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.74095 -0.53825 -0.09866  0.47084  1.89433 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.044655   0.353136  -0.126    0.899    \neval         0.272099   0.063346   4.295 2.13e-05 ***\nfemale      -0.279176   0.370222  -0.754    0.451    \nage         -0.024344   0.004535  -5.368 1.26e-07 ***\nfemale:age   0.008601   0.007735   1.112    0.267    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7402 on 458 degrees of freedom\nMultiple R-squared:  0.1267,    Adjusted R-squared:  0.1191 \nF-statistic: 16.62 on 4 and 458 DF,  p-value: 1e-12\n\n\n\n\n",
    "preview": "posts/2022-04-03-homework-2/HW2_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-04-23T11:37:50-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-03-homework-3/",
    "title": "Homework 3: Regression",
    "description": "\"Gelman Chapter 11, 12\"",
    "author": [
      {
        "name": "Isha Akshita Mahajan",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-04-03",
    "categories": [],
    "contents": "\n\n\nhide\n\nlibrary(tidyverse)\nlibrary(rstanarm)\nlibrary(bayesplot)\nlibrary(ggplot2)\n\n\n\n11.1 Assumptions of the Regression Model:\nFor the model in Section 7.1 predicting presidential vote share from the economy, discuss each of the assumptions in the numbered list in Section 11.1. For each assumption, state where it is made (implicitly or explicitly) in the model, whether it seems reasonable, and how you might address violations of the assumptions\n1.Validity: The Bread and Peace Model by Douglas Hibbs forecasts elections based on the economic growth. In this model the dependent variable is the election outcome which was measured as the incumbent party’s vote share and the independent variable is economic growth which is measured by average personal income growth. The measurement that this model includes is relevant to the the constructs they are trying to portray and not something vague and unrelated to broadly election outcome or the economic growth. As Gelman mentions in the book, using vote share is more informative than predicting a winner as this keeps things more generic. In this sense, I think the model is valid. One thing that I noticed is that the authors describe a “simple model” such as this to work very well in some cases however I wonder if just one predictor is enough to understand a large and common political phenomena like elections. What about the demographics of the voters whose average personal income growth was taken into account? What are their economic backgrounds? These are just some things I would think about while taking the sample for analysis in our data. Additionally, can this model be used to predict elections in other parts of the world? I think of this especially in the developing world, if we were to apply it in countries in Asia or Africa, will average personal income growth be truly reflective of their social fabric and the informal economies some of these countries have? While this model is valid on this data of US elections,I’d be curious to see how this can be used or tweaked around with other data.\n2.Representativeness: As Gelman writes in RaOS, the model is fit to data and is used to make inferences about a larger population– and also that the sample itself is representative of a larger population. In this case, the researchers aren’t really using a sample or a subset of a population so we might be able to eliminate that aspect of representativeness, but also the authors have not used years before 1952 when the US was going through drastic socio-economic and political changes due to the recession and WWII. Is it truly representative of all elections then? As Gelman writes in Ch 11, when there is some generalization involved, the issues of statistical significance arise and we need to ensure that the samples are representative of our corresponding populations in order to best make inferences with our generated fits.\n3.Additivity and Linearity: In this case, the model is linear as we are making an assumption that a unit increase in one things leads to increase in another, on average. In this model, there is only one predictor, and there are no transformations and interactions therefore it is safe to assume that additivity is not violated.\n4. Independence of Errors: The simple regression model assumes that errors from the prediction line are independent. Since the bread and peace model does not include elements of time-series, spatial analysis or multi-level settings, it is safe to assume that the errors from the prediction line are independent\n5. Equal Variance of Errors: This is a hetroscedastic model as there are non-equal variances in errors.To ensure that these constructs work in different settings, we would have to apply methods of scaling, transformation of perhaps even weights so that we can create a better fit for data that can be passed on to make inferences.\n11.5 Residuals and Predictions\nResiduals and predictions: The folder Pyth contains outcome y and predictors x1, x2 for 40 data points, with a further 20 points with the predictors but no observed outcome. Save the file to your working directory, then read it into R using read.table().\nUse R to fit a linear regression model predicting y from x1, x2, using the first 40 data points in the file. Summarize the inferences and check the fit of your model.\nDisplay the estimated model graphically as in Figure 10.2.\nMake a residual plot for this model. Do the assumptions appear to be met?\nMake predictions for the remaining 20 data points in the file. How confident do you feel about these predictions? After doing this exercise, take a look at Gelman and Nolan (2017, section 10.4) to see where these data came from**\n\n\nhide\n\nset.seed(007)\ndata <- read.table(\"Pyth.txt\",skip=1) \ndf<- data %>%\nslice(1:40)\ndf_1<- data %>%\nslice(41:60)\n\n\n\n\n\nhide\n\nfit<- lm(V1~ V2, data=df)\nsummary(fit)\n\n\n\nCall:\nlm(formula = V1 ~ V2, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.7409 -4.5056  0.7114  4.3739  7.7547 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  10.0633     1.5526   6.481 1.25e-07 ***\nV2            0.6559     0.2499   2.625   0.0124 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.921 on 38 degrees of freedom\nMultiple R-squared:  0.1535,    Adjusted R-squared:  0.1312 \nF-statistic:  6.89 on 1 and 38 DF,  p-value: 0.01242\n\n\n\nhide\n\nfit_1 <- lm (V1 ~ V3, data=df)\nsummary(fit_1)\n\n\n\nCall:\nlm(formula = V1 ~ V3, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1751 -1.2352 -0.1867  1.0899  5.3755 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3.78532    0.66037   5.732 1.33e-06 ***\nV3           0.83223    0.05017  16.589  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.863 on 38 degrees of freedom\nMultiple R-squared:  0.8787,    Adjusted R-squared:  0.8755 \nF-statistic: 275.2 on 1 and 38 DF,  p-value: < 2.2e-16\n\n\n\nhide\n\nfit_2<- lm(V1~ V2+V3, data=df)\nsummary(fit_2)\n\n\n\nCall:\nlm(formula = V1 ~ V2 + V3, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9585 -0.5865 -0.3356  0.3973  2.8548 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.31513    0.38769   3.392  0.00166 ** \nV2           0.51481    0.04590  11.216 1.84e-13 ***\nV3           0.80692    0.02434  33.148  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9 on 37 degrees of freedom\nMultiple R-squared:  0.9724,    Adjusted R-squared:  0.9709 \nF-statistic: 652.4 on 2 and 37 DF,  p-value: < 2.2e-16\n\nFrom this regression model, we see that the estimates of the slope coefficients and intercept estimates are statistically significant. For every 1 unit increase in x2, y increases by 0.3 on average. The R square value of 97.24 suggests that our linear fit explains 97.24 % of the variation in y values.\n\n\nhide\n\nplot(df$V2, df$V1, xlab = \"x1\", ylab= \"y\")\nabline(coef(fit))\n\n\n\n\n\n\nhide\n\nplot(df$V3, df$V1, xlab = \"x2\", ylab= \"y\")\nabline(coef(fit_1))\n\n\n\n\n\n\nhide\n\nsims_11.5 <- as.matrix(fit_2)\npredicted<- predict(fit_2)\nresidual<- df$V1-predicted\nplot(predicted, residual)\nabline(h=0, col= \"grey\")\n\n\n\n\nSince the points in residual plot are randomly scattered in comparison to the horizontal line, it seems that the model is correct.\n11.9 Leave-one-out cross validation:\nUse LOO to compare different models fit to the beauty and teaching evaluations example from Exercise 10.6:\nchecks your accuracy by seeing how well you predict your values, leave one and look at your regressions from problem set 3\nDiscuss the LOO results for the different models and what this implies, or should imply, for model choice in this example.\nCompare predictive errors point wise. Are there some data points that have high predictive errors for all the fitted models?\n\n\nhide\n\nbeauty<- read.csv(\"beauty.csv\")\n\nfit_beauty <- stan_glm(eval~beauty, data= beauty, refresh=0)\nprint(fit_beauty, digits=2)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      eval ~ beauty\n observations: 463\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 4.01   0.03  \nbeauty      0.13   0.03  \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.55   0.02  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\nhide\n\nfit_beauty1<- stan_glm(eval~beauty+female, data= beauty, refresh=0)\nprint(fit_beauty1, digits=2)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      eval ~ beauty + female\n observations: 463\n predictors:   3\n------\n            Median MAD_SD\n(Intercept)  4.09   0.03 \nbeauty       0.15   0.03 \nfemale      -0.20   0.05 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.54   0.02  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\nhide\n\nfit_beauty2<- stan_glm(eval~beauty+female+age+female:age, data= beauty, refresh=0)\nprint(fit_beauty2, digits=2)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      eval ~ beauty + female + age + female:age\n observations: 463\n predictors:   5\n------\n            Median MAD_SD\n(Intercept)  4.02   0.18 \nbeauty       0.14   0.03 \nfemale       0.34   0.28 \nage          0.00   0.00 \nfemale:age  -0.01   0.01 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.54   0.02  \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\nhide\n\nloo<- loo(fit_beauty)\nloo\n\n\n\nComputed from 4000 by 463 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -378.3 14.6\np_loo         2.9  0.3\nlooic       756.5 29.2\n------\nMonte Carlo SE of elpd_loo is 0.0.\n\nAll Pareto k estimates are good (k < 0.5).\nSee help('pareto-k-diagnostic') for details.\n\nhide\n\nloo1<- loo(fit_beauty1)\nloo1\n\n\n\nComputed from 4000 by 463 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -371.9 14.8\np_loo         4.0  0.4\nlooic       743.8 29.7\n------\nMonte Carlo SE of elpd_loo is 0.0.\n\nAll Pareto k estimates are good (k < 0.5).\nSee help('pareto-k-diagnostic') for details.\n\nhide\n\nloo2<- loo(fit_beauty2)\nloo2\n\n\n\nComputed from 4000 by 463 log-likelihood matrix\n\n         Estimate   SE\nelpd_loo   -371.1 15.1\np_loo         5.8  0.5\nlooic       742.1 30.3\n------\nMonte Carlo SE of elpd_loo is 0.0.\n\nAll Pareto k estimates are good (k < 0.5).\nSee help('pareto-k-diagnostic') for details.\n\n12.3 Scale of regression coefficients:\nA regression was fit to data from different countries, predicting the rate of civil conflicts given a set of geographic and political predictors. Here are the estimated coefficients and their z-scores (coefficient divided by standard error), given to three decimal places. Why are the coefficients for distance to border, distance to capital population, and GDP per capita so small?\nAns. The coefficient values for distance to border, distance to capital population and GDP per capita are 0.000 however, once we derive their z-scores, we are able to get estimates that are better comparable with one another, rather than creating a large window between the coefficients in the dataset.This speaks to the standardization approach where standard deviations can be seen as a measure of practical significance and that they roughly reflect the difference between the mean and a rough observation. In a way, this aligns the data into similar and more researchable units and gives the researcher a chance to compare a bundle of different predictors with different units. However, there are also things to keep in mind such as ensuring that the sample size is large enough etc.\n12.11 Elasticity:\nAn economist runs a regression examining the relations between the average price of cigarettes, P, and the quantity purchased, Q, across a large sample of counties in the United States, assuming the functional form, log Q = α + β log P. Suppose the estimate for β is 0.3. Interpret this coefficient.\nAs mentioned in RaOS, the coefficient of a log-log model is called elasticity. The coefficient in this case is interpreted as the expected proportional difference in y per proportional difference in x.In Ex. 12.11, (Proportionate (%) change in Q) / (Proportionate (%) change in P) = 0.3 therefore, for one unit proportionate change in p, there is 0.3 units of proportionate increase in Q. If the price of cigarettes P increases by 1% then the quantity purchased increases by 0.3%.\n12.9 Linear and logarithmic transformations:\nFor a study of congressional elections, you would like a measure of the relative amount of money raised by each of the two major-party candidates in each district. Suppose that you know the amount of money raised by each candidate; label these dollar values Di and Ri. You would like to combine these into a single variable that can be included as an input variable into a model predicting vote share for the Democrats. Discuss the advantages and disadvantages of the following measures:\n(a) The simple difference, Di − Ri: This relies on an assumption that a one unit increase in Di or decrease Ri will lead to an increase/decrease in y on average. This might not be useful as we don’t know what data looks like and also what other predictors we will be using in our model.\n(b) The ratio, Di/Ri: Using Di/Ri as a measure might not be a great choice as it will make the interpretation harder. Since it won’t be continuous, both values can take the value of zero, and it won’t be binary either, so this eliminates the possiblity of prediction at large.\n(c) The difference on the logarithmic scale, log Di − log Ri: Using the logarithmic scale is a choice that can only be made after we are aware of the spread of the data. In this, I would imagine that this won’t be required as the scales of measurement involved will be somewhat in alignment with one another. In addition, what if the difference results in a negative value?\n(d) The relative proportion, Di/(Di + Ri):  Seems to be a relatively better measure because the outcome we hope to predict is the vote share of democrats. In this case, we are aligning the predictor with our construct of interest, and also because we are looking\n12. 6 Logarithmic transformations:\nThe folder Pollution contains mortality rates and various environmental factors from 60 U.S. metropolitan areas (see McDonald and Schwing, 1973).\nFor this exercise we shall model mortality rate given nitric oxides, sulfur dioxide, and hydrocarbons as inputs. This model is an extreme oversimplification, as it combines all sources of mortality and does not adjust for crucial factors such as age and smoking. We use it to illustrate log transformations in regression.\nCreate a scatterplot of mortality rate versus level of nitric oxides. Do you think linear regression will fit these data well? Fit the regression and evaluate a residual plot from the regression.\nFind an appropriate transformation that will result in data more appropriate for linear regression. Fit a regression to the transformed data and evaluate the new residual plot.\nInterpret the slope coefficient from the model you chose in\nNow fit a model predicting mortality rate using levels of nitric oxides, sulfur dioxide, and hydrocarbons as inputs. Use appropriate transformations when helpful. Plot the fitted regression model and interpret the coefficients.\nCross validate: fit the model you chose above to the first half of the data and then predict for the second half. You used all the data to construct the model in (d), so this is not really cross validation, but it gives a sense of how the steps of cross validation can be implemented.\n\n\nhide\n\npollution<- read_csv(\"pollution.csv\")\nhead(pollution)\n\n\n# A tibble: 6 × 16\n   prec  jant  jult ovr65  popn  educ  hous  dens  nonw wwdrk  poor\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1    36    27    71   8.1  3.34  11.4  81.5  3243   8.8  42.6  11.7\n2    35    23    72  11.1  3.14  11    78.8  4281   3.5  50.7  14.4\n3    44    29    74  10.4  3.21   9.8  81.6  4260   0.8  39.4  12.4\n4    47    45    79   6.5  3.41  11.1  77.5  3125  27.1  50.2  20.6\n5    43    35    77   7.6  3.44   9.6  84.6  6441  24.4  43.7  14.3\n6    53    45    80   7.7  3.45  10.2  66.8  3325  38.5  43.1  25.5\n# … with 5 more variables: hc <dbl>, nox <dbl>, so2 <dbl>,\n#   humid <dbl>, mort <dbl>\n\n\n\nhide\n\nplot_pollution<- pollution %>% \n  ggplot(aes(x=nox,y=mort))+\n  geom_point()+\n   labs(x= \"Level of Nitric Oxides\", y= \"Mortality Rate\")+\n  theme(legend.position=\"bottom\")+\n  theme_minimal()+\n theme(text=element_text (size = 12,hjust = 0.5),\n       plot.subtitle = element_text(size = 10))\nplot_pollution\n\n\n\n\nRegression\n\n\nhide\n\nfit_pollution<- stan_glm(mort~nox, data = pollution, refresh=0)\nprint(fit_pollution, digits=2)\n\n\nstan_glm\n family:       gaussian [identity]\n formula:      mort ~ nox\n observations: 60\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 942.66   8.87\nnox          -0.10   0.18\n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 62.83   5.81 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\nResidual Plot\n\n\nhide\n\npredicted_1<- predict(fit_pollution)\nresid_1 <- pollution$mort- predicted_1\nplot(predicted_1, resid_1, xlab=\"Predicted value\", ylab=\"Residual\",\n     main=\"Residuals vs.Predicted Values\", mgp=c(1.5,.5,0), pch=20, yaxt=\"n\")\naxis(2, seq(-40,40,20), mgp=c(1.5,.5,0))\nabline(0, 0, col=\"gray\", lwd=.5)\n\n\n\n\n\n\n\n",
    "preview": "posts/2022-04-03-homework-3/HW3_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2022-04-23T12:01:46-04:00",
    "input_file": "HW3.knit.md"
  },
  {
    "path": "posts/2022-04-03-homework-4/",
    "title": "Homework 4: Logistic Regression",
    "description": "Logistic Regression, Gelman Chapter 13 , 14",
    "author": [
      {
        "name": "Isha Akshita Mahajan",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-04-03",
    "categories": [],
    "contents": "\nWorked with Ben, Nathaniel and Noah\nLoad required packages\n\n\nlibrary(rstanarm)\nlibrary(foreign)\nlibrary(tidyverse)\nlibrary(bayesplot)\n\n\n\nDescriptive Questions\n\nQuestion A. What is a logit function and what is its role in logistic regression?\n\nAnswer A The logit function maps the range (0,1) to (−∞, ∞) and is useful for modeling probabilities (RaOs). In logistic regression, this function is used to predict dependent variables that are binary instead of continuous. Rather than using log transformation, researchers use logistic regressions to keep information intact and predict in terms of probabilities as opposed to expected values which may go beyond 1 and below 0.\n\nB. Why do researchers typically not just use ordinary least squares regression (rather than logistic regression) when modeling a binary (dichotomous) response variable?\n\nAnswer B A linear regression will usually go below zero and above one, giving us lines rather the the S shaped curve. In the case of binary response/dependent variables, we’re mostly interested in the chance/ probability/ true or false/ 0 or 1 and the s shaped curve seems to be move intuitive because they don’t go beyond 0 and 1.\n\nC.What is the inverse logit function? How can it be used to interpret results from a logistic regression?\n\nAnswer C The inverse logit function logit−1(x) = ex 1 + ex. transforms the log of odds that we usually get with a regression to the probability in terms of 0s and 1s i.e. the transformation from linear predictors to probabilities used in a logistic regression. (Ra OS)\n\nD.Explain equations 13.2 and 13.3 on RaOS page 219 and the relationship between them.\n\nAnswer D\n13.1 Fitting Logistic Regression\nThe folder NES contains the survey data of presidential preference and income for the 1992 election analyzed in Section 13.1, along with other variables including sex, ethnicity, education, party identification, and political ideology.\nFit a logistic regression predicting support for Bush given all these inputs. Consider how to include these as regression predictors. (no interactions)\nEvaluate and compare the different models you have fit.\nFor your chosen model, discuss and compare the importance of each input variable in the prediction.\n\n\n#Load the data \nnes <- read.table(\"nes.txt\")\nhead(nes)\n\n\n    year resid weight1 weight2 weight3 age gender race educ1 urban\n536 1952     1       1       1       1  25      2    1     2     2\n537 1952     2       1       1       1  33      2    1     1     2\n538 1952     3       1       1       1  26      2    1     2     2\n539 1952     4       1       1       1  63      1    1     2     2\n540 1952     5       1       1       1  66      2    1     2     2\n541 1952     6       1       1       1  48      2    1     2     2\n    region income occup1 union religion educ2 educ3 martial_status\n536      1      4      2     1        1     3     3              1\n537      1      4      6     1        1     1     1              1\n538      1      3      6     2        2     3     3              1\n539      1      3      3     1        1     2     2              1\n540      2      1      6     2        1     4     4              1\n541      2      4      6     1        1     2     2              1\n    occup2 icpsr_cty fips_cty partyid7 partyid3 partyid3_b\n536      2        NA       NA        6        3          3\n537      6        NA       NA        5        3          3\n538      6        NA       NA        4        2          2\n539      3        NA       NA        7        3          3\n540      6        NA       NA        7        3          3\n541      6        NA       NA        3        1          1\n    str_partyid father_party mother_party dlikes rlikes dem_therm\n536           3            3            3      0      1        NA\n537           2            2            2     -1      3        NA\n538           1            1            1      0      5        NA\n539           4            1           NA     -1      3        NA\n540           4            1            1     -2      0        NA\n541           2            1            1      0      4        NA\n    rep_therm regis vote regisvote presvote presvote_2party\n536        NA     2    2         3        2               2\n537        NA     2    2         3        1               1\n538        NA     2    2         3        2               2\n539        NA     1    2         3        2               2\n540        NA     2    2         3        2               2\n541        NA     2    2         3        2               2\n    presvote_intent ideo_feel ideo7 ideo cd state inter_pre\n536               2        NA    NA   NA NA    13        50\n537               2        NA    NA   NA NA    13        50\n538               2        NA    NA   NA NA    13        50\n539               2        NA    NA   NA NA    13        50\n540               2        NA    NA   NA NA    24        49\n541               2        NA    NA   NA NA    24        49\n    inter_post black female age_sq rep_presvote rep_pres_intent south\n536         NA     0      1    625            1               1     0\n537         NA     0      1   1089            0               1     0\n538         NA     0      1    676            1               1     0\n539         NA     0      0   3969            1               1     0\n540         NA     0      1   4356            1               1     0\n541         NA     0      1   2304            1               1     0\n    real_ideo presapprov perfin1 perfin2 perfin presadm age_10\n536        NA         NA      NA      NA     NA      -1    2.5\n537        NA         NA      NA      NA     NA      -1    3.3\n538        NA         NA      NA      NA     NA      -1    2.6\n539        NA         NA      NA      NA     NA      -1    6.3\n540        NA         NA      NA      NA     NA      -1    6.6\n541        NA         NA      NA      NA     NA      -1    4.8\n    age_sq_10 newfathe newmoth parent_party white year_new income_new\n536  6.250000        1       1            2     1        1          1\n537 10.889999        0       0            0     1        1          1\n538  6.759999       -1      -1           -2     1        1          0\n539 39.690002       -1      NA           NA     1        1          0\n540 43.559998       -1      -1           -2     1        1         -2\n541 23.040001       -1      -1           -2     1        1          1\n      age_new vote.1 age_discrete race_adj dvote rvote\n536 -2.052455      1            1        1     0     1\n537 -1.252455      1            2        1     1     0\n538 -1.952455      1            1        1     0     1\n539  1.747545      1            3        1     0     1\n540  2.047545      1            4        1     0     1\n541  0.247545      1            3        1     0     1\n\n\n#clean data (used from RaOs code)\nok<- nes$year==1992 & !is.na(nes$rvote) & !is.na(nes$dvote) & (nes$rvote==1 | nes$dvote==1)\nnes92 <- nes[ok,]\nhead(nes92)\n\n\n      year resid weight1 weight2 weight3 age gender race educ1 urban\n32093 1992     1       1       1       1  41      2    4     2     2\n32094 1992     2       1       1       1  83      2    1     3     3\n32096 1992     4       1       1       1  67      2    2     2     1\n32097 1992     6       1       1       1  28      1    1     4     2\n32098 1992     8       1       1       1  71      2    4     2     2\n32099 1992     9       1       1       1  53      2    1     3     2\n      region income occup1 union religion educ2 educ3 martial_status\n32093      2      4      2     2        1     3     3              1\n32094      3      2      6     2        1     5     5              5\n32096      2      1      3     2        1     2     2              5\n32097      3      2      1     2        1     6     6              3\n32098      2      3      6     2        1     3     3              1\n32099      2      4      1     2        1     5     5              4\n      occup2 icpsr_cty fips_cty partyid7 partyid3 partyid3_b\n32093      2        NA    29095        7        3          3\n32094      6        NA    13031        5        3          3\n32096      3        NA    39113        1        1          1\n32097      1        NA    13135        5        3          3\n32098      6        NA    29095        4        2          2\n32099      1        NA    27003        1        1          1\n      str_partyid father_party mother_party dlikes rlikes dem_therm\n32093           4            3            3     -4     -2         0\n32094           2            1            1     -1      4        50\n32096           4            1            1      5     -5        85\n32097           2            1            1      0     -1        60\n32098           1           NA           NA      5     -5        85\n32099           4            3            3      4     -1        97\n      rep_therm regis vote regisvote presvote presvote_2party\n32093        97    NA    2         3        2               2\n32094        85    NA    2         3        2               2\n32096        40    NA    2         3        1               1\n32097        85    NA    2         3        2               2\n32098        30    NA    2         3        1               1\n32099        40    NA    2         3        1               1\n      presvote_intent ideo_feel ideo7 ideo cd state inter_pre\n32093               2        66     5    5  5    34        60\n32094               2        59     5    5  1    44        59\n32096               1        44     4    1  3    24        58\n32097               2        49     3    1  4    44        61\n32098               1        59     2    1  5    34        62\n32099               1        44     1    1  6    33        62\n      inter_post black female age_sq rep_presvote rep_pres_intent\n32093         55     0      1   1681            1               1\n32094          2     0      1   6889            1               1\n32096          9     1      1   4489            0               0\n32097          6     0      0    784            1               1\n32098          8     0      1   5041            0               0\n32099         30     0      1   2809            0               0\n      south real_ideo presapprov perfin1 perfin2 perfin presadm\n32093     0         5          1       1      NA      1       1\n32094     1         5          1       2      NA      2       1\n32096     0         4          2       2      NA      2       1\n32097     1         3          1       2      NA      2       1\n32098     0         2          2       2      NA      2       1\n32099     0         1          2       3      NA      3       1\n      age_10 age_sq_10 newfathe newmoth parent_party white year_new\n32093    4.1     16.81        1       1            2     0       20\n32094    8.3     68.89       -1      -1           -2     1       20\n32096    6.7     44.89       -1      -1           -2     0       20\n32097    2.8      7.84       -1      -1           -2     1       20\n32098    7.1     50.41       NA      NA           NA     0       20\n32099    5.3     28.09        1       1            2     1       20\n      income_new   age_new vote.1 age_discrete race_adj dvote rvote\n32093          1 -0.452455      1            2      1.5     0     1\n32094         -1  3.747545      1            4      1.0     0     1\n32096         -2  2.147545      0            4      2.0     1     0\n32097         -1 -1.752455      1            1      1.0     0     1\n32098          0  2.547545      0            4      1.5     1     0\n32099          1  0.747545      0            3      1.0     1     0\n\n\n\n# for getting Log odds and probability of the logistic regression results  \nlogit <- qlogis\ninvlogit <- plogis\n\n\n\n\n\nfit <-  stan_glm(rvote~educ1+female+partyid7+ideo7+black,family=binomial(link=\"logit\"), data=nes92,refresh=0)\nprint(fit, digits=2)\n\n\nstan_glm\n family:       binomial [logit]\n formula:      rvote ~ educ1 + female + partyid7 + ideo7 + black\n observations: 1132\n predictors:   6\n------\n            Median MAD_SD\n(Intercept) -6.82   0.56 \neduc1        0.13   0.11 \nfemale       0.34   0.20 \npartyid7     0.97   0.06 \nideo7        0.53   0.08 \nblack       -2.03   0.43 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\nfit_1 <-  stan_glm(rvote~educ1+female+partyid7,family=binomial(link=\"logit\"), data=nes92,refresh=0)\nprint(fit_1, digits=2)\n\n\nstan_glm\n family:       binomial [logit]\n formula:      rvote ~ educ1 + female + partyid7\n observations: 1177\n predictors:   4\n------\n            Median MAD_SD\n(Intercept) -4.60   0.39 \neduc1       -0.03   0.10 \nfemale       0.22   0.19 \npartyid7     1.09   0.06 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\nfit_2 <-  stan_glm(rvote~female+partyid7,family=binomial(link=\"logit\"), data=nes92,refresh=0)\nprint(fit_2, digits=2)\n\n\nstan_glm\n family:       binomial [logit]\n formula:      rvote ~ female + partyid7\n observations: 1177\n predictors:   3\n------\n            Median MAD_SD\n(Intercept) -4.70   0.28 \nfemale       0.23   0.19 \npartyid7     1.09   0.06 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n13.5 Interpreting logistic regression coefficients:\nHere is a fitted model from the Bangladesh analysis predicting whether a person with high-arsenic drinking water will switch wells, given the arsenic level in their existing well and the distance to the nearest safe well:\nstan_glm(formula = switch ~ dist100 + arsenic, family=binomial(link=“logit”), data=wells)\nMedian MAD_SD\n(Intercept) 0.00 0.08\ndist100 -0.90 0.10\narsenic 0.46 0.04\nCompare two people who live the same distance from the nearest well but whose arsenic levels differ, with one person having an arsenic level of 0.5 and the other person having a level of 1.0. You will estimate how much more likely this second person is to switch wells. Give an approximate estimate, standard error, 50% interval, and 95% interval, using two different methods:\nUse the divide-by-4 rule, based on the information from this regression output.\nUse predictive simulation from the fitted model in R, under the assumption that these two people each live 50 meters from the nearest safe well.\nAnswer (a) The divide by 4 rule on this regression output will used on the coefficient value of arsenic which is 0.46. When we divide 0.46/4, we get 0.115 which means that 1 unit more in arsenic concentration corresponds to an 11.5% positive difference in the probability of switching wells.\nAnswer (b)\n\n\nhead(wells)\n\n\n  switch arsenic   dist assoc educ\n1      1    2.36 16.826     0    0\n2      1    0.71 47.322     0    0\n3      0    2.07 20.967     0   10\n4      1    1.15 21.486     0   12\n5      1    1.10 40.874     1   14\n6      1    3.90 69.518     1    9\n\n#the first thing I would do is rescale the dist predictor to 50-meter units\n\nwells$dist50<- wells$dist/50\n\n\n\n\n13.7 Graphing a fitted logistic regression:\nWe downloaded data with weight (in pounds) and age (in years) from a random sample of American adults. We then defined a new variable: heavy <- weight > 200 and fit a logistic regression, predicting heavy from height (in inches):\nstan_glm(formula = heavy ~ height, family=binomial(link=“logit”), data=health)\nMedian MAD_SD\n(Intercept)-21.51 1.60 height 0.28 0.02\nGraph the logistic regression curve (the probability that someone is heavy) over the approximate range of the data. Be clear where the line goes through the 50% probability point.\nFill in the blank: near the 50% point, comparing two people who differ by one inch in height, you’ll expect a difference of 6.9%  in the probability of being heavy.\n\n\n#Part A \ncurve(invlogit(-21.51 + 0.28*x), ylim=c(0,1), xlim=c(40,110), ylab = \"Probability of Heavy\", xlab = \"Height (in inches\")\n\n\n\n\n#Part B \npart_b<-(invlogit(-21.51 + 0.28*77) - invlogit(-21.51 + 0.28*76))\npart_b\n\n\n[1] 0.06974525\n\n6.9% Code referenced from Nathan on Piazza\n14.3 Graphing logistic regressions:\nThe well-switching data described in Section 13.7 are in the folder Arsenic.\nFit a logistic regression for the probability of switching using log (distance to nearest safe well) as a predictor.\nMake a graph similar to Figure 13.8b displaying Pr(switch) as a function of distance to nearest safe well, along with the data.\nMake a residual plot and binned residual plot as in Figure 14.8.\n\n\n\n14.5 Working with logistic regression: (Use fakegrades.csv)\nFirst:\nFind sample mean and standard deviation of midterm grades; these are different than the numbers from the book.\nFit logistic regression using either glm( ) or stan_glm( ).\nUse the results to say something about the estimated relationship between midterm grade and the probability of passing the class.\nThen complete: (a) Graph the fitted model. Also on this graph put a scatterplot of hypothetical data consistent with the information given.\nSuppose the midterm scores were transformed to have a mean of 0 and standard deviation of 1, what would be the equation of the logistic regression using these transformed scores as a predictor?\n\n\ngrades<- read.csv(\"fakegrades.csv\")\nhead(grades)\n\n\n  ID pass midterm\n1  1    1      72\n2  2    0      49\n3  3    1      79\n4  4    0      53\n5  5    1      84\n6  6    1      75\n\n\n#part (i)\ngrades_mean<- mean(grades$midterm)\ngrades_sd<- sd(grades$midterm)\n#part(ii)\nfit_grades<- stan_glm(pass~midterm, family= binomial(link=\"logit\"), data=grades, refresh=0)\nprint(fit_grades)\n\n\nstan_glm\n family:       binomial [logit]\n formula:      pass ~ midterm\n observations: 50\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) -18.1    4.8 \nmidterm       0.3    0.1 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n#part(3) The regression returns a coefficient for 0.3. Since it is a logistic regression, We would use the divide by 4 rule. dividing 0.3 by 4 which returns 0.075. This means that with every increase of 1 unit or 1 point on the midterm grade, there is a 7.5% increase in the probability of the student passing\n#part(a)\n\n\nsims<- as.matrix(fit_grades)\nn_sims<- nrow(sims)\n  plot(grades$midterm,grades$pass, ylab = \"pass\", xlab=\"midterm\")\n  for(s in 1:50){\n    curve (invlogit (sims[s,1]+ sims[s,2]*x), lwd=1, add=TRUE)}\n\n\n\n\n\n\n\n",
    "preview": "posts/2022-04-03-homework-4/HW4_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2022-04-03T15:33:09-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-03-homework-5/",
    "title": "Homework 5: Model Interpretations",
    "description": "Types of Models: What is the Best Way to Analyze Less Frequent Forms of Violence? The Case of Sexual Aggression.",
    "author": [
      {
        "name": "Isha Akshita Mahajan",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-04-03",
    "categories": [],
    "contents": "\nREFERENCES: Swartout, Kevin M et al. “What is the Best Way to Analyze Less Frequent Forms of Violence? The Case of Sexual Aggression.” Psychology of violence vol. 5,3 (2015): 305-313. doi:10.1037/a0038316\nQuestion 1:\nWhat advantages do the authors claim count regressions—Poisson, negative binomial, and zero-inflated versions of each—have over OLS and simple transformations of variables?\nAnswer 1\nThe OLS regressions in the case of the SA data did not seem conclusive because the data is skewed. Using the square root transformation did not help much and only 4 out of 9 predictors came out to be statistically significant. Since the outcome of SA is in counts, the use of OLS isn’t much helpful here and the authors suggest the use of other count- models to perform such analysis.\nIn the case of Poisson, 7/9 covariates significantly predict SA frequency however, the results were in a low range. This is because the model did not account for the dispersion in the data and the values that would be zero i.e men who do not perpetrate SA and instead took into account overdispersed values of 1 and 2s.\nIn the case of the negative binomial, there were fewer significant predictors than the poisson however it highlighted the significant overdispersion, thus giving results that aligned more with the data.\nIn the case of the zero-inflated versions, the models allowed the researchers to take into account more accurately the men who did not perpetrate SA, i.e zero values. Given the skewed data and context to the situation, these models enable the researchers to take into account the zero values and predict frequencies keeping this group of people in mind.\nQuestion 2:\nWhat reason do they give for why a negative binomial regression may be superior to simple Poisson regression and what is the relationship between the two models?\nAnswers 2: The models in this paper were compared based on three criteria-\nOverall Fit- the AIC index and the Vueng test, Persimony- the number of estimated parameters taken into account and Interpretability- how coherent and easy was the model to interpret.\nBased on these, the negative binomial model outperformed the poisson model. The negative binomial model took into account the dispersion which showed to be significant in the results. The p values were reasonably low in the poisson regression and values that were artificially inflated. The negative binomial model on the other hand took into account the dispersion and broke the assumption of equidispersion made in the Poisson regression. The negative binomial was less parsimonious than the poisson model – given that it had an extra predictor of dispersion, the overall results showed a better fit for the data as it estimated the frequency of SA while relatively maintaining a high degree of parsimony i.e one additional predictor.\nQuestion 3:\nHow do “zero-inflated” versions of these models characterize individual respondents in a slightly different manner? Why might this be a useful approach?\nAnswer 3: The zero-inflated models allow the researchers to assume two distinct values- in this case a young male who does or does not perpetrate SA. In a dataset like this where certain men might be perpetrating higher SA while certain men might not at all, this model enables us to take into account a “true zero group”. Given that this article is highlighting characteristics of various models , this approach seems useful in datasets where the data is overdispersed. As presented by the authors, these models estimated the frequency distribution the closest, they used twice the amount of parameters than the standard negative binomial model, therefore proving to be highly parsimonious and maybe more impractical. However, keeping in mind the dispersion of the data, this might prove as a useful count based approach regardless, depending on the context in which it is being used.\nQuestion 4:\nInterpret coefficients (and standard errors). After reading over the descriptions of all variables and examining descriptive statistics in Table 1, do the following.\nFor the OLS model, interpret the coefficients for Charm, Sanctions, Impulsivity, and Often drunk and refer to the provided standard errors (SE) to provide added context.\nRepeat for the Poisson model.\nRepeat for the negative binomial model.\n(i) OLS model\nCharm: For unit increase in charm,there is an expected increase in SA by 0.05 units, on average Sanctions: For unit increase in sanctions, there is an expected decrease in SA by 0.039 units, on average Impulsivity: For unit increase in impulsivity, there is an expected increase in SA by 0.44 units, on average. Often Drunk :For unit increase in Often Drunk, there is an expected increase in SA by 0,75 units.\n(ii) Poisson model  Charm: For one unit increase in charm, given that other variables are held constant, the difference in logs of expected counts of SA is expected to increase by 0.21 units. Sanctions: For one unit increase in sanctions, given that other variables are held constant, the difference in logs of expected counts of SA is expected to increase by 0.28 units Impulsivity: For one unit increase in Impulsivity, given that other variables are held constant, the difference in logs of expected counts of SA is expected to increase by 0.33 units Often Drunk: For one unit increase in Often Drunk, given that other variables are held at constant, the difference in logs of expected counts of SA is expected to increase by 0.28 units.\n(iii) Negative Binomial Model Charm: For one unit increase in charm, given that other variables are held constant, the difference in logs of expected counts of SA is expected to increase by 0.23 units. Sanctions: For one unit increase in Sanctions, given that all variables are held constant, the difference in logs of expected counts of SA is expected to decrease by 0.20 units. Impulsivity: For one unit increase in Impulsivity, given that all variables are held constant, the difference in logs of expected counts of SA is expected to increase by 0.37 units. Often Drunk: For one unit increase in Often Drunk, given that all other variables are held constant, the difference in logs of expected counts of SA is expected to increase by 0.48 units.\nQuestion 5:\nOf the OLS, Poisson, and negative binomial regression models, which has the best fit and which has the worst fit (based on AIC scores)? What measure of model fit appeared in your analysis for homework 4 that plays a similar role as AIC?\nThe AIC Scores for all models were as follows:\nOLS - 1830 Poisson - 2824 Negative Binomial - 1129 Zero Inflated Poisson (ZIP) - 1484 Zero Inflated Negative Binomial (ZINB) -1108\nBased on the above results, the ZINB model has the best fit and the Poisson model has the worst fit. However, because the ZINB uses almost twice the amount of predictors the Negative Binomial Model does, the authors credit the Negative Binomial model to have the best fit. Going back to our homework four, we used the Leave Out One Cross Validation measure which played a similar role as the AIC. In order to check whether these interpretations are relevant to a different sample of the same population, these measures are used to validate the models.\nQuestion 6\nOf the four predictors you considered above, which seems to have the most consistent predictive difference across all three models (ignoring the zero-inflated versions)?\nAnswer 6: Out of the four predictors that we have considered above, Sanctions seems to have the most consistent predictive effects across all three models.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-04-23T11:39:56-04:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome: Introduction to Quantitative Analysis",
    "description": "Classwork from Introduction to Quantitative Analysis: Fall 2021",
    "author": [
      {
        "name": "Isha Akshita Mahajan",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-04-03",
    "categories": [],
    "contents": "\nHi There, Thank you for stopping by my first attempt to make a distill blog! As a part of DACSS 603: Introduction to Quantitative Analysis, this blog consists of my code for coursework in exploratory data analysis, regression modeling and data visualizations. Happy Browsing!\n\n\n\n",
    "preview": {},
    "last_modified": "2022-04-23T11:32:52-04:00",
    "input_file": {}
  }
]
