[
  {
    "path": "posts/welcome/",
    "title": "Welcome: Introduction to Quantitative Analysis",
    "description": "Classwork from Introduction to Quantitative Analysis: Fall 2021",
    "author": [
      {
        "name": "Isha Akshita Mahajan",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-04-03",
    "categories": [],
    "contents": "\nHi There, Thank you for checking out my blog. As a part of DACSS 603: Introduction to Quantitative Analysis, this blog consists of my code for coursework in exploratory data analysis, regression modeling and data visualizations. I hope you Enjoy!\n\n\n\n",
    "preview": {},
    "last_modified": "2022-04-03T15:01:28-04:00",
    "input_file": "welcome.knit.md"
  },
  {
    "path": "posts/2022-04-03-homework-1/",
    "title": "Homework 1: Function Writing, Variation",
    "description": "Gelman Chapter 6",
    "author": [
      {
        "name": "Isha Akshita Mahajan",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-04-03",
    "categories": [],
    "contents": "\nSetup Code:\n\n\n\n6.2 Programming Fake-Data Simulation: Write an R function to:\nsimulate n data point from the model, y= a+bx+error,with data points x uniformly sampled from the range(0,100) with errors drawn independently from the normal distribution with mean 0 and standard deviation σ*\nfit a linear regression to the simulated data*\nmake a scatterplot of the data and fitted line\n\n\n\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 8.2e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.82 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.049199 seconds (Warm-up)\nChain 1:                0.046578 seconds (Sampling)\nChain 1:                0.095777 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.3e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.065393 seconds (Warm-up)\nChain 2:                0.053475 seconds (Sampling)\nChain 2:                0.118868 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.1e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.069922 seconds (Warm-up)\nChain 3:                0.062212 seconds (Sampling)\nChain 3:                0.132134 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.4e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.14 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.050538 seconds (Warm-up)\nChain 4:                0.043311 seconds (Sampling)\nChain 4:                0.093849 seconds (Total)\nChain 4: \nstan_glm\n family:       gaussian [identity]\n formula:      y ~ x\n observations: 100\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 0.387  0.092 \nx           0.200  0.002 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.466  0.033 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n#6.3 Variation, uncertainty, and sample size: Repeat the example in Section 6.2, varying the number of data points, n. What happens to the parameter estimates and uncertainties when you increase the number of observations?\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.5e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.25 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.066285 seconds (Warm-up)\nChain 1:                0.071668 seconds (Sampling)\nChain 1:                0.137953 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 2.7e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.093037 seconds (Warm-up)\nChain 2:                0.072264 seconds (Sampling)\nChain 2:                0.165301 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.6e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.072375 seconds (Warm-up)\nChain 3:                0.07306 seconds (Sampling)\nChain 3:                0.145435 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.6e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.16 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.060577 seconds (Warm-up)\nChain 4:                0.051687 seconds (Sampling)\nChain 4:                0.112264 seconds (Total)\nChain 4: \nstan_glm\n family:       gaussian [identity]\n formula:      y ~ x\n observations: 175\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 0.434  0.070 \nx           0.200  0.001 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.471  0.026 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.8e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.050135 seconds (Warm-up)\nChain 1:                0.049118 seconds (Sampling)\nChain 1:                0.099253 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.082239 seconds (Warm-up)\nChain 2:                0.082301 seconds (Sampling)\nChain 2:                0.16454 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 2e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.063073 seconds (Warm-up)\nChain 3:                0.059982 seconds (Sampling)\nChain 3:                0.123055 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.053184 seconds (Warm-up)\nChain 4:                0.0533 seconds (Sampling)\nChain 4:                0.106484 seconds (Total)\nChain 4: \nstan_glm\n family:       gaussian [identity]\n formula:      y ~ x\n observations: 215\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 0.453  0.066 \nx           0.200  0.001 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.494  0.024 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1.9e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.086818 seconds (Warm-up)\nChain 1:                0.086089 seconds (Sampling)\nChain 1:                0.172907 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 4.1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.41 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.056136 seconds (Warm-up)\nChain 2:                0.05515 seconds (Sampling)\nChain 2:                0.111286 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.1e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.073266 seconds (Warm-up)\nChain 3:                0.064506 seconds (Sampling)\nChain 3:                0.137772 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.055684 seconds (Warm-up)\nChain 4:                0.052252 seconds (Sampling)\nChain 4:                0.107936 seconds (Total)\nChain 4: \nstan_glm\n family:       gaussian [identity]\n formula:      y ~ x\n observations: 275\n predictors:   2\n------\n            Median MAD_SD\n(Intercept) 0.473  0.062 \nx           0.199  0.001 \n\nAuxiliary parameter(s):\n      Median MAD_SD\nsigma 0.513  0.022 \n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\nAs I ran each code chunk by increasing the value of n in my sample, I observed that the MAD_SD values or the standard deviation started reducing. This suggests that a larger sample size leads to less error\n#6.5 Regression prediction and averages: The heights and earnings data in Section 6.3 are in the folder Earnings. Download the data and compute the average height for men and women in the sample.\nAssuming 52% of adults are women, estimate the average earnings of adults in the population.\nDirectly from the sample data compute the average earnings of men, women, and everyone. Compare these to the values calculated in parts (a) and (b)\n\n  height weight male  earn earnk ethnicity education mother_education\n1     74    210    1 50000    50     White        16               16\n2     66    125    0 60000    60     White        16               16\n3     64    126    0 30000    30     White        16               16\n4     65    200    0 25000    25     White        17               17\n5     63    110    0 50000    50     Other        16               16\n6     68    165    0 62000    62     Black        18               18\n  father_education walk exercise smokenow tense angry age\n1               16    3        3        2     0     0  45\n2               16    6        5        1     0     0  58\n3               16    8        1        2     1     1  29\n4               NA    8        1        2     0     0  57\n5               16    5        6        2     0     0  91\n6               18    1        1        2     2     2  54\n\n70.089 for men, 64.490 for women\nUse these averages and fitted regression model displayed on page 84 to get a model-based estimate of the average earnings of men and of women in the population.\n\n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 2.4e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.794078 seconds (Warm-up)\nChain 1:                0.175682 seconds (Sampling)\nChain 1:                0.96976 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 1.1e-05 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.530392 seconds (Warm-up)\nChain 2:                0.169498 seconds (Sampling)\nChain 2:                0.69989 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 1.2e-05 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.992653 seconds (Warm-up)\nChain 3:                0.170462 seconds (Sampling)\nChain 3:                1.16311 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'continuous' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 1.1e-05 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.11 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.284103 seconds (Warm-up)\nChain 4:                0.172834 seconds (Sampling)\nChain 4:                0.456937 seconds (Total)\nChain 4: \nstan_glm\n family:       gaussian [identity]\n formula:      earn ~ height + male\n observations: 1816\n predictors:   3\n------\n            Median    MAD_SD   \n(Intercept) -26037.06  12121.78\nheight         649.09    187.83\nmale         10567.39   1456.63\n\nAuxiliary parameter(s):\n      Median   MAD_SD  \nsigma 21398.62   346.92\n\n------\n* For help interpreting the printed output see ?print.stanreg\n* For info on the priors used see ?prior_summary.stanreg\n\n\n\n\n",
    "preview": "posts/2022-04-03-homework-1/HW1_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-04-03T15:11:45-04:00",
    "input_file": "HW1.knit.md"
  },
  {
    "path": "posts/2022-04-03-homework-2/",
    "title": "Homework 2: Exploratory Data Analysis",
    "description": "Exploratory Data Analysis on Voter Turnout and Ethnic Data",
    "author": [
      {
        "name": "Isha Akshita Mahajan",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2022-04-03",
    "categories": [],
    "contents": "\nLoad Required Packages\n\n\n\nQuestion 1\nFraga analyzes turnout data for four different racial and ethnic groups, but for this analysis we will focus on the data for black voters. Load blackturnout.csv. Which years are included in the dataset? How many different states are included in the dataset?\n\n# A tibble: 6 × 7\n   ...1  year state district turnout   CVAP candidate\n  <dbl> <dbl> <chr>    <dbl>   <dbl>  <dbl>     <dbl>\n1     1  2008 AK           0   0.710 0.0350         0\n2     2  2010 AK           0   0.448 0.0323         0\n3     3  2010 AK           1   0.448 0.0323         0\n4     4  2008 AK           1   0.710 0.0350         0\n5     5  2006 AK           1   0.439 0.0318         0\n6     6  2010 AL           0   0.397 0.256          0\n\nThis dataset includes data from years 2006,2008 and 2010 and includes information on voter turnout from 42 states\nQuestion 2:\nCreate a boxplot that compares turnout in elections with and without a co-ethnic candidate.Be sure to use informative labels.Interpret the resulting graph.\n\n\n\nThis boxplot shows the difference between the turnout for elections with and without co-ethnic candidates. The median for black voter turnout is lower for non co ethnic candidates whereas there is a higher turnout in elections where there is presence of co ethnic candidates. This can be considered as a step towards correlation however,this boxplot can not represent causality on it’s own.\nQuestion 3:\nRun a linear regression with black turnout as your outcome variable and candidate co-ethnicity as your predictor. Report the coefficient on your predictor and the intercept. Interpret these coefficients. Do not merely comment on the direction of the association (i.e.,whether the slope is positive or negative). Explain what the value of the coefficients mean in terms of the units in which each variable is measured. Based on these coefficients, what would you conclude about blacks voter turnout and co-ethnic candidates?\n\n\nCall:\nlm(formula = turnout ~ candidate, data = voter_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.3164 -0.1282 -0.0436  0.1191  0.5832 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.393857   0.005183  75.993  < 2e-16 ***\ncandidate   0.061640   0.014984   4.114 4.15e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.171 on 1235 degrees of freedom\nMultiple R-squared:  0.01352,   Adjusted R-squared:  0.01272 \nF-statistic: 16.92 on 1 and 1235 DF,  p-value: 4.149e-05\n\nThis model summarizes the difference in average black voter turnout between elections that have black candidates (1) and elections that don’t (0). The intercept, 0.39, is the average or predicted proportion of black voting age population in a district that votes in a general election when the election does not include a co-ethnic/black candidate. To average the proportion of black voting age population when the election does include a co-ethnic/black candidate, we would add 0.06 i.e. 0.39+0.06 to get 0.45 which is the predicted turnout of elections with co-ethnic candidates. This tells us that the turnout for election with black candidate is on average, 0.06 units higher than the one without a co-ethnic candidates. This regression has a positive slope and can be interpreted as a summary of the difference in average voter turnout for elections with and without black candidates.\nQuestion 4:\nYou decide to investigate the results of the previous question a bit more carefully because the elections with co-ethnic candidates may differ from the elections without co-ethnic candidates in other ways. Create a scatter plot where the x-axis is the proportion of co-ethnic voting-age population and the y-axis is black voter turnout. Color your points according to candidate co-ethnicity. That is, make the points for elections featuring co-ethnic candidates one color, and make the points for elections featuring no co-ethnic candidates a different color. Interpret the graph.\n\n\n\nThis scatterplot shows that when there no black candidates running for elections, the proportion of eligible voters who show up to vote is relatively lower to when there is an election that includes a co-ethnic candidate.This can be considered as an additional step towards analyzing black voter turnout, however, we can not make strong causal claims.\nI would like to expand on Clustering around low CVAP and low turnout but I’m still a little confused on that part\nQuestion 5:\nRun a linear regression with black turnout as your outcome variable and with candidate co-ethnicity and co-ethnic voting-age population as your predictors.Report the coefficients, including the intercept. Interpret the coefficients on the two predictors, ignoring the intercept for now (you will interpret the intercept in the next question). Explain what each coefficient represents in terms of the units of the relevant variables.\n\n\nCall:\nlm(formula = turnout ~ candidate + CVAP, data = voter_data, refresh = 0)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.30534 -0.12775 -0.04529  0.11750  0.59576 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.375275   0.006677  56.203  < 2e-16 ***\ncandidate   -0.007364   0.021703  -0.339    0.734    \nCVAP         0.207392   0.047497   4.366 1.37e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1698 on 1234 degrees of freedom\nMultiple R-squared:  0.02853,   Adjusted R-squared:  0.02695 \nF-statistic: 18.12 on 2 and 1234 DF,  p-value: 1.756e-08\n\nFor every one unit of increase in the proportion of district’s voting age population that is black, the black voter turnout increases by 0.21 units, on average, given there are no co-ethnic candidates.In the regression results above, we see that the coefficient for candidate is negative and insignificant (no stars), therefore the value does not hold much relevance in the model. In the earlier regression with only one predictor-turnout, there was an average increase in black voter turnout by 0.06 units, however, with the addition of another predictor,the value of that seemed to have diminished.\nQuestion 6:\nNow interpret the intercept from the regression model with two predictors. Is this intercept a substantively important or interesting quantity? Why or why not?\nThe intercept in an interesting quantity for two reasons:First, the regression results show that it is a significant value therefore it shall be an integral part of the model. Secondly, On average, the intercept is 0.37 which in comparison to the model with one predictor is only 0.02 units lower. This comparison shows that the intercept is in alignment with the fit regardless of the turnout predictor\nQuestion 7:\nRelationship between co-ethnic candidates and black voter turnout. Based on regression model with two predictors, what do you conclude about the relationship between co-ethnic candidates and black voter turnout. Ignore issues of statistical significance.\nKeeping aside the significance, the coefficient value of candidate is -0.007 which is very minuscule and therefore does not have a significant impact on the intercept and the model itself. I thought this to be in alignment with null hypothesis and might contribute towards making an argument for that stronger?\nQuestions From RaOS\n10.2 Regression with Interactions\nWrite the equation of the estimated regression line of y on x for the treatment group and the control group, and the equation of the estimated regression line of y on x for the control group.\nGraph with pen on paper the two regression lines, assuming the values of x fall in the range (0, 10). On this graph also include a scatterplot of data (using open circles for treated units and dots for controls) that are consistent with the fitted model.\n\nPart A\n\nx=1.6 (Pre Treatment Predictor) z= 2.7 (Treatment Indicator) x:z = 0.7 sigma = 0.5\nWhen z=0, then y= a+bx+cz+d(x:z) = 1.2+1.6x+2.7(0)+0.7(x)(0) = 1.2+1.6x\nWhen z=1, then y= 1.6x+1.2+2.7(1)+0.7(x)(1) = 3.9+2.3x\nIn Treatment, for every one unit increase in x, y increases by an average of 0.7 units in control group.\n\nPART B\n\n\n\n\nQuestion 10.3\n\n\nCall:\nlm(formula = var2 ~ var1, data = fake)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1708 -0.6723  0.0279  0.5928  3.4590 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept)  0.01832    0.03222   0.569    0.570\nvar1         0.02115    0.03245   0.652    0.515\n\nResidual standard error: 1.019 on 998 degrees of freedom\nMultiple R-squared:  0.0004253, Adjusted R-squared:  -0.0005763 \nF-statistic: 0.4247 on 1 and 998 DF,  p-value: 0.5148\n\n\n\nCall:\nlm(formula = var4 ~ var3, data = fake_1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2702 -0.6763 -0.0161  0.6761  2.8278 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)\n(Intercept) -0.01769    0.03142  -0.563    0.574\nvar3         0.03460    0.03185   1.086    0.278\n\nResidual standard error: 0.9934 on 998 degrees of freedom\nMultiple R-squared:  0.001181,  Adjusted R-squared:  0.0001804 \nF-statistic:  1.18 on 1 and 998 DF,  p-value: 0.2776\n\n\n\nCall:\nlm(formula = var6 ~ var5, data = fake_2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.5125 -0.7104  0.0315  0.7026  3.7598 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)\n(Intercept)  0.034537   0.031892   1.083    0.279\nvar5        -0.005155   0.031275  -0.165    0.869\n\nResidual standard error: 1.008 on 998 degrees of freedom\nMultiple R-squared:  2.722e-05, Adjusted R-squared:  -0.0009748 \nF-statistic: 0.02717 on 1 and 998 DF,  p-value: 0.8691\n\nQuestion 10.6\nRegression models with interactions: The folder Beauty contains data (use file beauty.csv) Beauty and teaching evaluations from Hamermesh and Parker (2005) on student evaluations of instructors’ beauty and teaching quality for several courses at the University of Texas. The teaching evaluations were conducted at the end of the semester, and the beauty judgments were made later, by six students who had not attended the classes and were not aware of the course evaluations.\n\nPART A\n\nRun a regression using beauty (the variable beauty) to predict course evaluations (eval), adjusting for various other predictors. Graph the data and fitted model, and explain the meaning of each of the coefficients along with the residual standard deviation. Plot the residuals versus fitted values.\n\n\nCall:\nlm(formula = eval ~ beauty, data = beauty)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.80015 -0.36304  0.07254  0.40207  1.10373 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.01002    0.02551 157.205  < 2e-16 ***\nbeauty       0.13300    0.03218   4.133 4.25e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5455 on 461 degrees of freedom\nMultiple R-squared:  0.03574,   Adjusted R-squared:  0.03364 \nF-statistic: 17.08 on 1 and 461 DF,  p-value: 4.247e-05\n\n\nThe regression above shows that for every one unit increase in beauty, the evaluations increase by 0.13 units, on average. Both the intercept and the beauty variable hold statistical significance in the model.\n\nPART B\n\nFit some other models, including beauty and also other predictors. Consider at least one model with interactions. For each model, explain the meaning of each of its estimated coefficients.\n\n\nCall:\nlm(formula = eval ~ beauty + female, data = beauty)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.87196 -0.36913  0.03493  0.39919  1.03237 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.09471    0.03328  123.03  < 2e-16 ***\nbeauty       0.14859    0.03195    4.65 4.34e-06 ***\nfemale      -0.19781    0.05098   -3.88  0.00012 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5373 on 460 degrees of freedom\nMultiple R-squared:  0.0663,    Adjusted R-squared:  0.06224 \nF-statistic: 16.33 on 2 and 460 DF,  p-value: 1.407e-07\n\nWhen beauty=0, i.e. male, the female coefficient is -0.20 which shows a decrease of 0.20 units in evaluations on average\n\n\nCall:\nlm(formula = beauty ~ eval + female + age + female:age, data = beauty)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.74095 -0.53825 -0.09866  0.47084  1.89433 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.044655   0.353136  -0.126    0.899    \neval         0.272099   0.063346   4.295 2.13e-05 ***\nfemale      -0.279176   0.370222  -0.754    0.451    \nage         -0.024344   0.004535  -5.368 1.26e-07 ***\nfemale:age   0.008601   0.007735   1.112    0.267    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7402 on 458 degrees of freedom\nMultiple R-squared:  0.1267,    Adjusted R-squared:  0.1191 \nF-statistic: 16.62 on 4 and 458 DF,  p-value: 1e-12\n\n\n\n\n",
    "preview": "posts/2022-04-03-homework-2/HW2_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2022-04-03T15:16:35-04:00",
    "input_file": "HW2.knit.md"
  }
]
